DATASETS:
  TRAIN: ("refcocog_train_umd", )
  TEST: ("refcocog_val_umd", )
MODEL:
  META_ARCHITECTURE: "OpenWorldSAM2"
#  WEIGHTS: "" # for training from scratch
  WEIGHTS: "checkpoints/model_final_refcocog.pth" # for evaluation
  PIXEL_MEAN: [ 123.675, 116.280, 103.530 ]
  PIXEL_STD: [ 58.395, 57.120, 57.375 ]
  OpenWorldSAM2:
    MASK_WEIGHT: 20.0
    DICE_WEIGHT: 0.0
    # EVF-SAM2 config
    QUERY_DIM: 256
    TORCH_DTYPE: "fp32"
    TRAIN_TIE_BREAKER: True
    TRAIN_VLM: False
    USE_VISUAL_TOKENS: True  # Control whether to use visual tokens in VLM
    USE_CROSS_ATTENTION: True  # Enable cross-attention between VLM features and image embeddings
    CROSS_ATTENTION_LAYERS: 3
    ENCODER_PRETRAINED: "checkpoints/beit3_large_patch16_224.pth"
    VISION_PRETRAINED: "checkpoints/sam2_hiera_large.pt"
    NUM_OBJECT_QUERIES: 20
    TEST:
      SEMANTIC_ON: False
      INSTANCE_ON: False
      PANOPTIC_ON: False
      REFER_ON: True
      TOP_K_ON: False
      NMS_ON: False
      NMS_THRESHOLD: 0.5
      IOU_THRESHOLD: 0.0
      TWO_STAGE_INFERENCE: False
INPUT:
  IMAGE_SIZE: 1024
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "refcoco"
SOLVER:
  AMP:
    ENABLED: True
  CHECKPOINT_PERIOD: 5000
  MAX_ITER: 100000 # 500000
  IMS_PER_BATCH: 8
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.005  # More aggressive clipping
    NORM_TYPE: 2.0
TEST:
  EVAL_PERIOD: 5000 #50000
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: True
  RANDOM_SUBSET_RATIO: 0.001
  NUM_WORKERS: 8
VERSION: 2